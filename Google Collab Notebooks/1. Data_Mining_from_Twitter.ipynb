{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Mining**"
      ],
      "metadata": {
        "id": "Z1UuiliOWF2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing  Snscrape"
      ],
      "metadata": {
        "id": "BUZwkeAdLjNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git #(data mining package)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaZ-GEK0WWrx",
        "outputId": "f32618a1-f2e3-4d56-a196-688488a08330"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
            "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /tmp/pip-req-build-6t7n68jr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git /tmp/pip-req-build-6t7n68jr\n",
            "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 3dd9c28e31b8babeb2a187fbae994d9717ded168\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from snscrape==0.6.2.20230321.dev3+g3dd9c28) (3.10.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from snscrape==0.6.2.20230321.dev3+g3dd9c28) (4.9.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from snscrape==0.6.2.20230321.dev3+g3dd9c28) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from snscrape==0.6.2.20230321.dev3+g3dd9c28) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->snscrape==0.6.2.20230321.dev3+g3dd9c28) (2.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (1.7.1)\n",
            "Building wheels for collected packages: snscrape\n",
            "  Building wheel for snscrape (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for snscrape: filename=snscrape-0.6.2.20230321.dev3+g3dd9c28-py3-none-any.whl size=72466 sha256=a5f4beb51a0e350f1282ca160e511dfdf0c0bd9c50cfd35028103d3f34347c06\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-msbeq_3x/wheels/1a/ba/e2/39fa3a11802c4a622f2efc8be3f5ff854481051d0b4c95c1fd\n",
            "Successfully built snscrape\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.6.2.20230321.dev3+g3dd9c28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining  Function"
      ],
      "metadata": {
        "id": "4szeD9osQGPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_tweet(limit,search,since,until):\n",
        "  \"\"\" \n",
        "  Download limit no. of tweets per day\n",
        "  from since till until from twitter \n",
        "  with query and returns a dataframe\n",
        "  \"\"\"\n",
        "\n",
        "  main_container = []\n",
        "\n",
        "  dates = pd.date_range(start = since, end = until)\n",
        "\n",
        "  for i in dates:\n",
        "    query = ''\n",
        "    query += search\n",
        "    date = i.date().strftime(\"%Y-%m-%d\")\n",
        "    query += \" until:\"\n",
        "    query += date\n",
        "    query += \" exclude:retweets exclude:replies\"\n",
        "    mini_container = []\n",
        "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "      if len(mini_container) == limit:\n",
        "          break\n",
        "      else:\n",
        "        if (tweet.lang=='en'):\n",
        "          mini_container.append([tweet.user.username, tweet.date, tweet.rawContent])\n",
        "    main_container.extend(mini_container)\n",
        "  \n",
        "  df = pd.DataFrame(main_container, columns=[\"User\", \"Date Created\", \"Tweet\"])\n",
        "  return df"
      ],
      "metadata": {
        "id": "jvRXNjoDX_Z2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training data"
      ],
      "metadata": {
        "id": "etur5tL1QM8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "limit = 55 #tweets per day\n",
        "search = \"Facebook\"\n",
        "since = '1/2/2018' #month/date/year\n",
        "until = '4/8/2023' #month/date/year\n",
        "training_df = scrape_tweet(limit,search,since,until)\n",
        "training_df.to_csv('Fb_Training.csv', index=False)"
      ],
      "metadata": {
        "id": "CiniMYhgOywQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before Layoff data"
      ],
      "metadata": {
        "id": "qpHt7NVUQNr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "limit = 30 #tweets per day\n",
        "search = \"Facebook\"\n",
        "since = '1/2/2022' #month/date/year\n",
        "until = '4/8/2023' #month/date/year\n",
        "before_df = scrape_tweet(limit,search,since,until)\n",
        "before_df.to_csv('Before_Layoff.csv', index=False)"
      ],
      "metadata": {
        "id": "-OjMrkzKb2OQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "before_df['Tweet'].unique().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749857b1-fe90-4317-ff99-97f4b383820e",
        "id": "0zaBTaqWqGAA"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13150,)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate = before_df[before_df.duplicated()]\n",
        "print(duplicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06008e7-1582-423f-941d-0fbe03209686",
        "id": "x5A933vyqGAB"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  User              Date Created  \\\n",
            "13477  UmpaLoompaLarry 2023-03-26 23:58:38+00:00   \n",
            "\n",
            "                                                   Tweet  \n",
            "13477  you need to be our fan on Facebook. and you ne...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Layoff data"
      ],
      "metadata": {
        "id": "NncAtrAuQOcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "limit = 80 #tweets per day\n",
        "search = \"Facebook Layoff\"\n",
        "since = '11/2/2022' #month/date/year\n",
        "until = '4/8/2023' #month/date/year\n",
        "after_df = scrape_tweet(limit,search,since,until)\n",
        "after_df.to_csv('After_Layoff.csv', index=False)"
      ],
      "metadata": {
        "id": "KTqYqdtD25L1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "after_df['User'].unique().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu1-HJMUTJXg",
        "outputId": "029e5061-6ff2-42a7-ceb1-8ec73ab2c8ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2850,)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate = after_df[after_df.duplicated()]\n",
        "print(duplicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shK8eK2oponk",
        "outputId": "76e676cc-2fd3-4e52-9bcb-7fa90143c41e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  User              Date Created  \\\n",
            "85     jaihobharatnews 2022-11-01 22:52:25+00:00   \n",
            "86        TheOpenChain 2022-11-01 14:18:30+00:00   \n",
            "87            danspena 2022-11-01 11:43:00+00:00   \n",
            "88        TheLayoffBot 2022-11-01 11:11:07+00:00   \n",
            "89            danspena 2022-11-01 03:08:00+00:00   \n",
            "...                ...                       ...   \n",
            "12635          YiLiYou 2023-04-02 12:19:06+00:00   \n",
            "12636         danspena 2023-04-02 10:43:00+00:00   \n",
            "12637        AgeAsimov 2023-04-02 07:03:27+00:00   \n",
            "12638         danspena 2023-04-02 02:08:00+00:00   \n",
            "12639         danspena 2023-04-01 10:43:00+00:00   \n",
            "\n",
            "                                                   Tweet  \n",
            "85     ‘False': Elon Musk on Twitter layoffs to avoid...  \n",
            "86     Hot News 🔥 Tech talent migrates to #Web3 as la...  \n",
            "87     BUSINESS INSIDER: Facebook is conducting 'quie...  \n",
            "88     https://t.co/CkXNNTUJnx\\nFacebook\\nLatest Gues...  \n",
            "89     BUSINESS INSIDER: Facebook is conducting 'quie...  \n",
            "...                                                  ...  \n",
            "12635  Disney Begins Mass Layoffs THIS Week https://t...  \n",
            "12636  BUSINESS INSIDER: Facebook is conducting 'quie...  \n",
            "12637  One man's dumbness cost the employment of thou...  \n",
            "12638  BUSINESS INSIDER: Facebook is conducting 'quie...  \n",
            "12639  BUSINESS INSIDER: Facebook is conducting 'quie...  \n",
            "\n",
            "[8934 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Layoff Genral data"
      ],
      "metadata": {
        "id": "WCyWCr1c9TOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "limit = 80 #tweets per day\n",
        "search = \"Facebook\"\n",
        "since = '11/2/2022' #month/date/year\n",
        "until = '4/8/2023' #month/date/year\n",
        "after_general_df = scrape_tweet(limit,search,since,until)\n",
        "after_general_df.to_csv('After_Layoff_General.csv', index=False)"
      ],
      "metadata": {
        "id": "hyNWYdlp9TOr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "after_general_df['User'].unique().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1300f9e4-f674-48fe-8a48-999ce15a0c4a",
        "id": "pFPTisAE9TOs"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10568,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate = after_general_df[after_general_df.duplicated()]\n",
        "print(duplicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98cad297-1da8-4be5-85c0-95aa91cccd3f",
        "id": "If6qYJV69TOs"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  User              Date Created  \\\n",
            "11607  UmpaLoompaLarry 2023-03-26 23:58:38+00:00   \n",
            "\n",
            "                                                   Tweet  \n",
            "11607  you need to be our fan on Facebook. and you ne...  \n"
          ]
        }
      ]
    }
  ]
}