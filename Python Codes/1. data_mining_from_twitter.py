# -*- coding: utf-8 -*-
"""Data Mining from Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uPKfZWtiOnm7MostSbiKIoPpeSiwoyM5

## **Data Mining**

### Installing  Snscrape
"""

!pip install git+https://github.com/JustAnotherArchivist/snscrape.git #(data mining package)

"""### Defining  Function"""

import snscrape.modules.twitter as sntwitter
import pandas as pd

def scrape_tweet(limit,search,since,until):
  """ 
  Download limit no. of tweets per day
  from since till until from twitter 
  with query and returns a dataframe
  """

  main_container = []

  dates = pd.date_range(start = since, end = until)

  for i in dates:
    query = ''
    query += search
    date = i.date().strftime("%Y-%m-%d")
    query += " until:"
    query += date
    query += " exclude:retweets exclude:replies"
    mini_container = []
    for tweet in sntwitter.TwitterSearchScraper(query).get_items():
      if len(mini_container) == limit:
          break
      else:
        if (tweet.lang=='en'):
          mini_container.append([tweet.user.username, tweet.date, tweet.rawContent])
    main_container.extend(mini_container)
  
  df = pd.DataFrame(main_container, columns=["User", "Date Created", "Tweet"])
  return df

"""### Training data"""

limit = 55 #tweets per day
search = "Facebook"
since = '1/2/2018' #month/date/year
until = '4/8/2023' #month/date/year
training_df = scrape_tweet(limit,search,since,until)
training_df.to_csv('Fb_Training.csv', index=False)

"""### Before Layoff data"""

limit = 30 #tweets per day
search = "Facebook"
since = '1/2/2022' #month/date/year
until = '4/8/2023' #month/date/year
before_df = scrape_tweet(limit,search,since,until)
before_df.to_csv('Before_Layoff.csv', index=False)

before_df['Tweet'].unique().shape

duplicate = before_df[before_df.duplicated()]
print(duplicate)

"""### After Layoff data"""

limit = 80 #tweets per day
search = "Facebook Layoff"
since = '11/2/2022' #month/date/year
until = '4/8/2023' #month/date/year
after_df = scrape_tweet(limit,search,since,until)
after_df.to_csv('After_Layoff.csv', index=False)

after_df['User'].unique().shape

duplicate = after_df[after_df.duplicated()]
print(duplicate)

"""### After Layoff Genral data"""

limit = 80 #tweets per day
search = "Facebook"
since = '11/2/2022' #month/date/year
until = '4/8/2023' #month/date/year
after_general_df = scrape_tweet(limit,search,since,until)
after_general_df.to_csv('After_Layoff_General.csv', index=False)

after_general_df['User'].unique().shape

duplicate = after_general_df[after_general_df.duplicated()]
print(duplicate)